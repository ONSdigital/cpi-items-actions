{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrylau/Documents/GitHub/cpi-items-actions/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request, json \n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import openpyxl\n",
    "from datetime import datetime,date\n",
    "\n",
    "# if you want to make notebooks have really long outputs\n",
    "# pd.options.display.max_rows=4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save timestamp to text file\n",
    "with open('timestamp.txt', 'w') as f:\n",
    "    f.write(datetime.now().strftime(\"%Y-%b-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you need to install something in jupyter notebokes, you need these commands\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you need to create the metadata file from the starting file, (drops the ID_NAME, dedupes CS_CODES etc)\n",
    "\n",
    "# Read the Excel file\n",
    "df_meta = pd.read_excel('2025_starting_file_test_data.xlsx')\n",
    "\n",
    "# Drop columns after \"AVERAGE_PRICE\"\n",
    "if 'AVERAGE_PRICE' in df_meta.columns:\n",
    "    last_col_idx = df_meta.columns.get_loc('AVERAGE_PRICE')\n",
    "    df_meta = df_meta.iloc[:, :last_col_idx + 1]\n",
    "\n",
    "# metadata for datadownload\n",
    "df_meta.to_csv('2025_all_items_metadata.csv', index=False)\n",
    "\n",
    "# Drop the \"ID name\" column\n",
    "df_meta = df_meta.drop(columns=['ID_NAME'])\n",
    "\n",
    "# Rearrange so CONSUMPTION_SEGMENT_CODE is the first column\n",
    "if 'CONSUMPTION_SEGMENT_CODE' in df_meta.columns:\n",
    "    cols = ['CONSUMPTION_SEGMENT_CODE'] + [col for col in df_meta.columns if col != 'CONSUMPTION_SEGMENT_CODE']\n",
    "    df_meta = df_meta[cols]\n",
    "\n",
    "# Dedupe by \"CONSUMPTION_SEGMENT_CODE\"\n",
    "df_meta = df_meta.drop_duplicates(subset=['CONSUMPTION_SEGMENT_CODE'])\n",
    "\n",
    "# Save as \"2025_metadata.csv\"\n",
    "df_meta.to_csv('2025_metadata.csv', index=False)\n",
    "\n",
    "\n",
    "### if you need to create the unchained file from the starting file which keeps just the CS and unchained values\n",
    "\n",
    "# Read the CSV file\n",
    "df_start = pd.read_csv('2025_starting_file_test_data.csv')\n",
    "\n",
    "# Select columns: CONSUMPTION_SEGMENT_CODE and columns from '202101' onwards\n",
    "cols_to_keep = ['CONSUMPTION_SEGMENT_CODE'] + [col for col in df_start.columns if re.match(r'^2021\\d{2}$', col) or re.match(r'^20\\d{4}$', col) and col >= '202101']\n",
    "unchained2025 = df_start[cols_to_keep]\n",
    "\n",
    "# Dedupe by \"CONSUMPTION_SEGMENT_CODE\"\n",
    "unchained2025 = unchained2025.drop_duplicates(subset=['CONSUMPTION_SEGMENT_CODE'])\n",
    "\n",
    "# Remove hyphens from all values in the DataFrame\n",
    "unchained2025 = unchained2025.replace('-', '', regex=True)\n",
    "\n",
    "# Save as \"2025_unchained.csv\"\n",
    "unchained2025.to_csv('2025_unchained.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As we only track prices for 5 years, you need to set that time point as a reference point.\n",
    "# Also as we are using the last January as a reference point, we need to set that as well.\n",
    "\n",
    "# set average price reference month\n",
    "avgpriceRefMonth=pd.Timestamp('2025-01-01 00:00:00')\n",
    "\n",
    "# starting reference point\n",
    "startref=pd.Timestamp('2020-01-01 00:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in metadata\n",
    "meta = pd.read_csv('./2025_metadata.csv',parse_dates=['ID_START'],date_format=\"%Y%m\")\n",
    "meta[\"CONSUMPTION_SEGMENT_CODE\"] = meta[\"CONSUMPTION_SEGMENT_CODE\"].astype(str)\n",
    "meta = meta.set_index(\"CONSUMPTION_SEGMENT_CODE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to split a string at a certain occurance of a separator\n",
    "\n",
    "# https://stackoverflow.com/questions/36300158/split-text-after-the-second-occurrence-of-character\n",
    "def split(strng, sep, pos):\n",
    "    strng = strng.split(sep)\n",
    "    return sep.join(strng[:pos]), sep.join(strng[pos:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 2, 1, 0, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in unchained csv\n",
    "unchained = pd.read_csv('2025_unchained.csv')\n",
    "unchained[\"CONSUMPTION_SEGMENT_CODE\"] = unchained[\"CONSUMPTION_SEGMENT_CODE\"].astype(str)\n",
    "unchained = unchained.set_index(\"CONSUMPTION_SEGMENT_CODE\")\n",
    "\n",
    "#find the last month in the unchained file\n",
    "# latestmonth=datetime.strptime(unchained.columns[-1],\"%Y-%m-%d %H:%M:%S\")\n",
    "# latestmonth=datetime.strptime(unchained.columns[-1],\"%Y-%m-%d\")\n",
    "latestmonth=datetime.strptime(unchained.columns[-1],\"%Y%m\")\n",
    "# and print it out\n",
    "latestmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Excel changes date formats into something funky. Use this if you need to convert it to python date format\n",
    "# un = pd.read_csv('unchained.csv', index_col=0)\n",
    "# columns = {}\n",
    "# for col in un.columns:\n",
    "#     try:\n",
    "#         columns[col] = datetime.strptime(str(col), \"%d/%m/%Y\").date()\n",
    "#         # columns[col] = datetime.strptime(str(col), \"%d/%m/%Y %H:%M\").date()\n",
    "#         # columns[col] = datetime.strptime(str(col), \"%Y-%m-%d %H:%M:%S\").date()\n",
    "#         # columns[col] = datetime.strptime(str(col), \"%Y%m\").date()\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "# un.rename(columns=columns, inplace=True)\n",
    "# un.to_csv('unchained.csv',date_format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to merge the next month test data\n",
    "\n",
    "# get a test file \n",
    "df = pd.read_csv('march_2025_example_test_data.csv')\n",
    "df['CS_ID'] = df['CS_ID'].astype(str)\n",
    "\n",
    "index_date=df.iloc[0,0]\n",
    "\n",
    "# parse columns as dates in unchained\n",
    "# https://stackoverflow.com/questions/42472418/parse-file-headers-as-date-objects-in-python-pandas\n",
    "columns = {}\n",
    "for col in unchained.columns:\n",
    "    try:\n",
    "        columns[col] = datetime.strptime(str(col), \"%Y%m\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "unchained.rename(columns=columns, inplace=True)\n",
    "\n",
    "un = unchained.merge(\n",
    "    df[['CS_ID', 'CPI_INDEX']].rename(columns={\"CPI_INDEX\": datetime.strptime(str(index_date), \"%Y%m\")}),\n",
    "    left_on=\"CONSUMPTION_SEGMENT_CODE\",\n",
    "    right_on='CS_ID',\n",
    "    how='inner'\n",
    ")\n",
    "un = un.rename(columns={\"CS_ID\":\"CONSUMPTION_SEGMENT_CODE\"})\n",
    "    \n",
    "#if last date is Jan, then chain it to december\n",
    "if(un.columns[-1].month == 1):\n",
    "    print('chaining jan')\n",
    "    jancol=un.columns[-1]\n",
    "    prevdec=un.columns[-2]\n",
    "    for index,value in un.iloc[:,-1].items():\n",
    "        un.at[index,jancol]=un.loc[index,prevdec]*value/100\n",
    "\n",
    "un.set_index(\"CONSUMPTION_SEGMENT_CODE\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the /data.json from the cpi items and prices page from the ONS website\n",
    "with requests.Session() as s:\n",
    "    r=s.get(\"https://www.ons.gov.uk/economy/inflationandpriceindices/datasets/consumerpriceindicescpiandretailpricesindexrpiitemindicesandpricequotes/data\",headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    data = r.json()\n",
    "    datasets = data['datasets']\n",
    "\n",
    "#go through the dataset and find the first one which doesn't contain the word framework, glossary or /pricequotes. The url includes pricesquotes so that slash is important. Save the index as the variable match  \n",
    "for i,dataset in enumerate(datasets):\n",
    "    match = i\n",
    "    if('framework' not in dataset['uri'] and 'glossary' not in dataset['uri'] and '/pricequotes' not in dataset['uri']):\n",
    "        break\n",
    "    \n",
    "#get the uri of the items dataset we want\n",
    "items = data['datasets'][match]['uri']\n",
    "print('dataset='+items)\n",
    "\n",
    "#get the month and year from the uri\n",
    "date=split(items,'itemindices',2)[1]\n",
    "print('the date from url:'+date)\n",
    "\n",
    "#parse it as a date\n",
    "itemmonth=datetime.strptime(date,\"%B%Y\")\n",
    "\n",
    "# check date to see if you need to download a file\n",
    "if(itemmonth!=latestmonth):\n",
    "    print('month from indices is different to latest month in unchained csv')\n",
    "    \n",
    "    with requests.Session() as s:\n",
    "        r=s.get(\"https://www.ons.gov.uk\"+items+\"/data\",headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        itemspage = r.json()\n",
    "        csv = itemspage['downloads'][0]['file']\n",
    "    \n",
    "    # get the csv of the latest indices\n",
    "    with requests.Session() as s:\n",
    "        download = s.get(\"https://www.ons.gov.uk/file?uri=\"+items+\"/\"+csv,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        df=pd.read_csv(io.StringIO(download.content.decode('utf-8')))\n",
    "    \n",
    "    #get the index date which is the first cell\n",
    "    index_date=df.iloc[0,0]\n",
    "    \n",
    "    # parse columns as dates in unchained\n",
    "    # https://stackoverflow.com/questions/42472418/parse-file-headers-as-date-objects-in-python-pandas\n",
    "    columns = {}\n",
    "    for col in unchained.columns:\n",
    "        try:\n",
    "            columns[col] = datetime.strptime(str(col), \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    unchained.rename(columns=columns, inplace=True)\n",
    "    \n",
    "    #join it onto existing csv\n",
    "    un=unchained.merge(df[['ITEM_ID','ALL_GM_INDEX']].rename(columns={\"ALL_GM_INDEX\": datetime.strptime(str(index_date),\"%Y%m\")}),on='ITEM_ID',how='left')\n",
    "    \n",
    "    #if last date is Jan, then chain it to december\n",
    "    if(un.columns[-1].month == 1):\n",
    "        print('chaining jan')\n",
    "        jancol=un.columns[-1]\n",
    "        prevdec=un.columns[-2]\n",
    "        for index,value in un.iloc[:,-1].items():\n",
    "            un.at[index,jancol]=un.loc[index,prevdec]*value/100\n",
    "    \n",
    "    un.set_index(\"ITEM_ID\",inplace=True)\n",
    "\n",
    "else:\n",
    "    print('Nothing to update')  \n",
    "    # parse columns as dates in unchained\n",
    "    # https://stackoverflow.com/questions/42472418/parse-file-headers-as-date-objects-in-python-pandas\n",
    "    columns = {}\n",
    "    for col in unchained.columns:\n",
    "        try:\n",
    "            columns[col] = datetime.strptime(str(col), \"%Y-%m-%d\")\n",
    "            # columns[col] = datetime.strptime(str(col), \"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    unchained.rename(columns=columns, inplace=True)\n",
    "    un=unchained\n",
    "    un.set_index(\"ITEM_ID\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['520130', '520131', '520137', '520140', '510439', '510434', '520109', '510429', '510432', '510324', '510328', '510330', '510336', '510340', '510344', '510347', '510348', '510349', '510350', '510351', '510352', '510436', '510437', '510438', '510353', '510528', '510104', '510106', '510108', '510109', '510113', '510117', '510118', '510120', '510128', '510130', '510131', '510132', '510133', '510134', '510413', '510419', '510433', '510501', '510515', '510516', '510535', '510534', '510206', '510208', '510212', '510215', '510219', '510223', '510233', '510235', '510236', '510237', '510245', '510248', '510250', '510257', '510258', '510259', '510405', '510406', '510407', '510506', '510529', '510530', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, '520138', '520139', '520235', '520242', '520251', '520213', '520216', '520223', '520226', '520236', '520237', '520241', '520243', '520247', '520250', '520255', '430211', '430216', '430224', '430233', '430234', '430235', '430237', '430425', '430422', '520118', '430509', '430529', '430533', '430540', '430542', '430543', '430507', '430531', '430414', '430304', '430407', '430423', '430419', '430505', '430307', '430317', '430351', '430356', '430358', '430362', '430524', '430541', '630166', '430229', '430236', '520136', '430101', '430104', '430119', '430120', '430127', '430133', '430134', '430137', '430138', '430139', '430141', '430142', '430143', '430144', '520132', '410602', '410607', '410611', '410614', '410621', '410625', '410629', '410631', '410632', '410633', '410637', '420103', '420405', '430228', '430413', '430539', '430525', '430526', '430527', '430530', '430535', '430409', '430503', '520134', '630156', '630159', '630160', '630164', '630434', '630437', '630439', '630440', '630226', '630376', '630314', '630317', '630342', '630359', '630368', '630373', '630382', '630505', '630507', '630510', '630518', '630519', '630525', '630526', '640212', nan, '640406', '430613', '430617', '430621', '430622', '430626', '430627', '630340', '630343', '630345', '630357', '640219', '640222', '640224', '640226', '640243', '640244', '440120', '440126', '440128', '440129', '440232', '520324', '520325', '520301', '520303', '520311', '520313', '520326', '520332', '410508', '410509', '410517', '410518', '440101', '440113', '440116', '440118', '440104', '440105', '440121', '440123', '440127', '440227', '440240', '440254', '520331', '620318', '620319', '610115', '610227', '610231', '610232', '610235', '610236', '610238', '610242', '620308', '620315', '610204', '610303', '620303', '620307', 'CP0111101', 'CP0111301', 'CP0111302', 'CP0111303', 'CP0111304', 'CP0230101', 'CP0230201', 'CP0230901']\n"
     ]
    }
   ],
   "source": [
    "print(un.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create a copy of unchained to create the chained indices\n",
    "chained = un.copy()\n",
    "\n",
    "for col in chained:\n",
    "    for i, row_value in chained[col].items():\n",
    "        # print(col,i,row_value,meta.loc[i,'ID_START'])\n",
    "        if(col>=meta.loc[i,'ID_START']):\n",
    "            if(col==startref):\n",
    "                chained.at[i,col]=100\n",
    "            # elif(col==meta.loc[i,'ITEM_START']):\n",
    "            #     sample.at[i,col]=row_value\n",
    "            elif(col<=startref+pd.tseries.offsets.DateOffset(years=1)):\n",
    "                chained.at[i,col]=row_value\n",
    "            else:\n",
    "                if(col.month==1 and col>startref+pd.tseries.offsets.DateOffset(years=1)):\n",
    "                    chained.at[i,col]=float(row_value)*float(chained.loc[i][datetime(col.year-1,1,1)])/100\n",
    "                else:\n",
    "                    chained.at[i,col]=float(row_value)*float(chained.loc[i][datetime(col.year,1,1)])/100\n",
    "\n",
    "        elif(col==meta.loc[i,'ID_START']-pd.tseries.offsets.DateOffset(months=1)):\n",
    "            chained.at[i,col]=100\n",
    "\n",
    "        else:\n",
    "            chained.at[i,col]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then calculate average prices\n",
    "# avgprice=chained.copy()\n",
    "\n",
    "# for col in avgprice:\n",
    "#     for i, row_value in avgprice[col].items():\n",
    "#         if(row_value==None):\n",
    "#             avgprice.at[i,col]=None\n",
    "#         else:\n",
    "#             avgprice.at[i,col]=float(row_value)/ \\\n",
    "#             float(chained.loc[i,avgpriceRefMonth])* \\\n",
    "#             float(meta.loc[i,'AVERAGE_PRICE'])\n",
    "            \n",
    "# #rename columns to dates without time formats\n",
    "# columns = {}\n",
    "# for col in avgprice.columns:\n",
    "#     try:\n",
    "#         columns[col] = col.date()\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "# avgprice.rename(columns=columns, inplace=True)\n",
    "\n",
    "# avgprice.astype(float).round(2).to_csv('2025_avgprice_cs.csv',date_format='%Y-%m-%d',na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate annual growth for CS\n",
    "# annualgrowth=chained.copy()\n",
    "\n",
    "# for col in annualgrowth:\n",
    "#     for i, row_value in annualgrowth[col].items():\n",
    "#         prev_col = col - pd.tseries.offsets.DateOffset(years=1)\n",
    "#         if prev_col in annualgrowth.columns:\n",
    "#             # safe to access previous year\n",
    "#             annualgrowth.at[i, col] = (\n",
    "#                 float(row_value) - float(chained.loc[i, prev_col])\n",
    "#             ) * 100 / float(chained.loc[i, prev_col])\n",
    "#         else:\n",
    "#             annualgrowth.at[i, col] = None\n",
    "                \n",
    "                \n",
    "# #rename columns to dates without time formats\n",
    "# columns = {}\n",
    "# for col in annualgrowth.columns:\n",
    "#     try:\n",
    "#         columns[col] = col.date()\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "# annualgrowth.rename(columns=columns, inplace=True)\n",
    "                \n",
    "# annualgrowth.astype(float).round(0).astype(int,errors='ignore').to_csv('2025_annualgrowth_cs.csv',date_format='%Y-%m-%d',na_rep='',float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate monthly growth for cs\n",
    "# monthlygrowth=chained.copy()\n",
    "\n",
    "# for col in monthlygrowth:\n",
    "#     for i, row_value in monthlygrowth[col].items():\n",
    "#         prev_col = col - pd.tseries.offsets.DateOffset(months=1)\n",
    "#         if (col < meta.loc[i, 'ID_START']) or (prev_col not in chained.columns):\n",
    "#             monthlygrowth.at[i, col] = None\n",
    "#         else:\n",
    "#             monthlygrowth.at[i, col] = (\n",
    "#                 (float(row_value) - float(chained.loc[i, prev_col])) * 100\n",
    "#                 / float(chained.loc[i, prev_col])\n",
    "#             )\n",
    "\n",
    "# #rename columns to dates without time formats\n",
    "# columns = {}\n",
    "# for col in monthlygrowth.columns:\n",
    "#     try:\n",
    "#         columns[col] = col.date()\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "# monthlygrowth.rename(columns=columns, inplace=True)\n",
    "                \n",
    "# monthlygrowth.astype(float).round(0).astype(int,errors='ignore').to_csv('2025_monthlygrowth_cs.csv',date_format='%Y-%m-%d',na_rep='',float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally save the unchained and chainedvnumbers to csv\n",
    "#rename columns to dates without time formats\n",
    "columns = {}\n",
    "for col in un.columns:\n",
    "    try:\n",
    "        columns[col] = col.date()\n",
    "    except ValueError:\n",
    "        pass\n",
    "un.rename(columns=columns, inplace=True)\n",
    "\n",
    "#and save it\n",
    "un.to_csv('unchained.csv')\n",
    "\n",
    "#rename columns to dates without time formats\n",
    "columns = {}\n",
    "for col in chained.columns:\n",
    "    try:\n",
    "        columns[col] = col.date()\n",
    "    except ValueError:\n",
    "        pass\n",
    "chained.rename(columns=columns, inplace=True)\n",
    "\n",
    "chained.astype(float).round(3).to_csv('chained.csv',date_format='%Y-%m-%d',na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn it into a excel datadownload file\n",
    "# with pd.ExcelWriter(\"datadownload.xlsx\", mode=\"a\", if_sheet_exists=\"replace\", date_format=\"YYYY-MM-DD\", datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "#     meta.drop(columns=['AVERAGE_PRICE']).to_excel(writer, sheet_name=\"metadata\")  \n",
    "#     # un.to_excel(writer, sheet_name=\"unchained\")\n",
    "#     chained.astype(float).round(3).transpose().to_excel(writer, sheet_name=\"chained\")\n",
    "#     avgprice.astype(float).round(2).fillna('').transpose().to_excel(writer, sheet_name=\"averageprice\")\n",
    "#     monthlygrowth.astype(float).round(0).fillna('').transpose().to_excel(writer, sheet_name=\"monthlygrowth\")\n",
    "#     annualgrowth.astype(float).round(0).fillna('').transpose().to_excel(writer,sheet_name=\"annualgrowth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ID_NAME and CONSUMPTION_SEGMENT_CODE from allitems with avgprice calculated from chained indices\n",
    "\n",
    "# Ensure allitems, chained, and avgprice_allitems are loaded\n",
    "allitems = pd.read_csv('2025_all_items_metadata.csv')\n",
    "\n",
    "# Ensure chained index is CONSUMPTION_SEGMENT_CODE\n",
    "if chained.index.name != 'CONSUMPTION_SEGMENT_CODE':\n",
    "    chained = chained.reset_index().set_index('CONSUMPTION_SEGMENT_CODE')\n",
    "\n",
    "# Prepare avgprice DataFrame with ID_NAME and CONSUMPTION_SEGMENT_CODE\n",
    "avgprice_merged = allitems[['ID_NAME', 'CONSUMPTION_SEGMENT_CODE']].copy()\n",
    "\n",
    "# Calculate avgprice for each segment and month using chained indices and AVERAGE_PRICE\n",
    "for col in chained.columns:\n",
    "    avgprice_merged[str(col)] = avgprice_merged['CONSUMPTION_SEGMENT_CODE'].map(\n",
    "        lambda seg: round((chained.loc[seg, col] / chained.loc[seg, chained.columns[0]] * allitems.loc[allitems['CONSUMPTION_SEGMENT_CODE'] == seg, 'AVERAGE_PRICE'].values[0]), 2)\n",
    "        if seg in chained.index and not allitems.loc[allitems['CONSUMPTION_SEGMENT_CODE'] == seg, 'AVERAGE_PRICE'].empty else None\n",
    "    )\n",
    "\n",
    "# Save to CSV\n",
    "avgprice_merged.to_csv('avgprice_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.read_excel(\"2025_starting_file_test_data.xlsx\")\n",
    "df_meta[\"CONSUMPTION_SEGMENT_CODE\"].astype(str).eq(\"510439\").any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate monthly growth of chained indices for each segment, rounded to integers\n",
    "# Uses ID_NAME and CONSUMPTION_SEGMENT_CODE from allitems\n",
    "\n",
    "import math\n",
    "\n",
    "# Ensure allitems and chained are loaded\n",
    "allitems = pd.read_csv('2025_all_items_metadata.csv')\n",
    "if chained.index.name != 'CONSUMPTION_SEGMENT_CODE':\n",
    "    chained = chained.reset_index().set_index('CONSUMPTION_SEGMENT_CODE')\n",
    "\n",
    "# Prepare DataFrame for monthly growth\n",
    "monthly_growth = allitems[['ID_NAME', 'CONSUMPTION_SEGMENT_CODE']].copy()\n",
    "\n",
    "# Calculate monthly growth for each segment and month, handle NaN safely\n",
    "for idx, col in enumerate(list(chained.columns)[1:], start=1):\n",
    "    prev_col = chained.columns[idx-1]\n",
    "    def calc_growth(seg):\n",
    "        try:\n",
    "            prev = chained.loc[seg, prev_col]\n",
    "            curr = chained.loc[seg, col]\n",
    "            if pd.isna(prev) or pd.isna(curr) or prev == 0:\n",
    "                return None\n",
    "            return int(round((curr - prev) * 100 / prev))\n",
    "        except Exception:\n",
    "            return None\n",
    "    monthly_growth[str(col)] = monthly_growth['CONSUMPTION_SEGMENT_CODE'].map(calc_growth)\n",
    "\n",
    "# Save to CSV\n",
    "monthly_growth.to_csv('monthly_growth_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate annual growth of chained indices for each segment, rounded to integers (0 decimal places)\n",
    "# Uses ID_NAME and CONSUMPTION_SEGMENT_CODE from allitems\n",
    "\n",
    "# Ensure allitems and chained are loaded\n",
    "allitems = pd.read_csv('2025_all_items_metadata.csv')\n",
    "if chained.index.name != 'CONSUMPTION_SEGMENT_CODE':\n",
    "    chained = chained.reset_index().set_index('CONSUMPTION_SEGMENT_CODE')\n",
    "\n",
    "# Prepare DataFrame for annual growth\n",
    "annual_growth = allitems[['ID_NAME', 'CONSUMPTION_SEGMENT_CODE']].copy()\n",
    "\n",
    "# Calculate annual growth for each segment and month (from 12th column onwards)\n",
    "for idx, col in enumerate(list(chained.columns)[12:], start=12):\n",
    "    prev_col = chained.columns[idx-12]\n",
    "    def calc_annual_growth(seg):\n",
    "        try:\n",
    "            prev = chained.loc[seg, prev_col]\n",
    "            curr = chained.loc[seg, col]\n",
    "            if pd.isna(prev) or pd.isna(curr) or prev == 0:\n",
    "                return None\n",
    "            return int(round((curr - prev) * 100 / prev))\n",
    "        except Exception:\n",
    "            return None\n",
    "    annual_growth[str(col)] = annual_growth['CONSUMPTION_SEGMENT_CODE'].map(calc_annual_growth)\n",
    "\n",
    "# Save to CSV\n",
    "annual_growth.to_csv('annual_growth_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn it into a excel datadownload file\n",
    "\n",
    "meta_for_datadownload = pd.read_csv(\"2025_all_items_metadata.csv\")\n",
    "\n",
    "with pd.ExcelWriter(\"datadownload.xlsx\", mode=\"a\", if_sheet_exists=\"replace\", date_format=\"YYYY-MM-DD\", datetime_format=\"YYYY-MM-DD\") as writer:\n",
    "    meta_for_datadownload.drop(columns=[\"AVERAGE_PRICE\"]).to_excel(writer, index=False, sheet_name=\"Metadata\")  \n",
    "    # un.to_excel(writer, sheet_name=\"unchained\")\n",
    "    \n",
    "    # make it tidy, join on meta data, reorder columns by index\n",
    "    chained.round(3).reset_index().melt(id_vars=['CONSUMPTION_SEGMENT_CODE'],var_name='Date',value_name='Value').dropna()\\\n",
    "    .merge(meta_for_datadownload.reset_index()[[\"ID_NAME\",'CONSUMPTION_SEGMENT_NAME','CONSUMPTION_SEGMENT_CODE','Category1','Category2','WEIGHT\\SIZE']])\\\n",
    "    .iloc[:,[1,3,4,0,5,6,2]]\\\n",
    "    .to_excel(writer, index=False, sheet_name=\"Chained\")\n",
    "    \n",
    "    avgprice_merged.round(2).melt(id_vars=[\"ID_NAME\",'CONSUMPTION_SEGMENT_CODE'], var_name='Date', value_name='Price').dropna(subset=['Price'])\\\n",
    "    .merge(meta_for_datadownload.reset_index()[[\"ID_NAME\",'CONSUMPTION_SEGMENT_NAME','CONSUMPTION_SEGMENT_CODE','Category1','Category2','WEIGHT\\SIZE']])\\\n",
    "    .iloc[:,[2,0,4,1,5,6,7,3]]\\\n",
    "    .to_excel(writer, index=False, sheet_name=\"Average price\")\n",
    "    \n",
    "    monthly_growth.round(0).melt(id_vars=[\"ID_NAME\",'CONSUMPTION_SEGMENT_CODE'],var_name='Date',value_name='Percentage').dropna(subset=['Percentage'])\\\n",
    "    .merge(meta_for_datadownload.reset_index()[[\"ID_NAME\",'CONSUMPTION_SEGMENT_NAME','CONSUMPTION_SEGMENT_CODE','Category1','Category2','WEIGHT\\SIZE']])\\\n",
    "    .iloc[:,[2,0,4,1,5,6,7,3]]\\\n",
    "    .to_excel(writer, index=False, sheet_name=\"Monthly growth\")\n",
    "    \n",
    "    annual_growth.round(0).melt(id_vars=[\"ID_NAME\",'CONSUMPTION_SEGMENT_CODE'],var_name='Date', value_name='Percentage').dropna(subset=['Percentage'])\\\n",
    "    .merge(meta_for_datadownload.reset_index()[[\"ID_NAME\",'CONSUMPTION_SEGMENT_NAME','CONSUMPTION_SEGMENT_CODE','Category1','Category2','WEIGHT\\SIZE']])\\\n",
    "    .iloc[:,[2,0,4,1,5,6,7,3]]\\\n",
    "    .to_excel(writer,index=False, sheet_name=\"Annual growth\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
